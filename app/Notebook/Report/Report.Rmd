---
title: "End Year Report"
author: "Loong Kuan"
date: "13 January 2017"
---

```{r setup, echo = FALSE, message = FALSE}
source("../../drift_timeline/timeline.R")
```

# Aim
Previously, the project was only able to analyse the changes between 2 given sets of data.
However, this requires the user to know and split their dataset at the point where drift occurs beforehand.

Therefore, during the trip I worked towards adding functionality in order to show any occurences of concept drift in an entire data set or stream.

# Outline of Work Done
The first thing I realised was that the old model I used, `FrequencyTable`, was inadequate for calculating the drift magnitude over an entire data set or stream in a timeley manner.
Therefore, I created a new model, `FrequencyMaps`, which through experimentation, seems to be faster at just calculating the drift magnitude over an entire data set/stream.

With a faster model, I then developed a simple method to analyse the drift magnitude over an entire data set.
Testing this method on synthetic data showed that it was sufficient to obtain an estimate of where abrupt concept drift occurs in a data set.

With a new method to estimate where drift occurs in a data set, I went back and analysed the concept drift in data sets that, previously, I had only analysed through splitting the data set in half.

# New Model for streaming data (FrequencyMaps)

## FrequqencyTable (recap)

Previously, the project used a `FrequencyTable` to store the frequencies at which different instances appear in the given dataset.

![Figure 1: `FrequencyTable`](FrequencyTable.png)

The `FrequencyTable` allows for new instances to be added in linear time to the number of attributes in the data set.
Furthermore, the frequency of partial instances of different attribute subset lengths can be queried from the same `FrequencyTable`.

However, in order to obtain the frequency of a single partial instance from a `FrequencyTable`, we need to either iterate through each row of the table or all the possible values of the attributes not given a value in the partial instance. 
This time complexity is amplified further by the fact there is a combinatorial number of attribute subsets for a given attribute subset length with each attribute subset having an exponential number of partial instances.

In order to reduce time taken to obtain all the frequencies, I created a new model that trades Space Complexity for Time. Specifically, it increases space complexity by an combinatorial factor to reduce final time complexity by an exponential factor.

![Figure 2: `FrequencyMap`](FrequencyMap.png)

The model works by storing the `FrequencyTables` of each possible attribute subset of a given attribute subset length.
So, given a partial instance to find the frequency of, the model will first look at the attributes in the partial instace that are given values.
Based on that, it will access the appropiate `FrequencyTable` and find the frequency of the partial instance by hashing the partial instance.

Therefore, not only is this model limited to obtaining the frequencies of a fixed attribute subset length, but it has a greater time complexity for adding instances and greater space complexity.

## Comparison
![Figure 3: Comparison between `FrequencyTable` and `FrequencyMap`](compare.png)

# Finding drift in datasets
In order to measure the drift magnitude over an entire data set, 
we can utilise a simple method that measures the drift magnitude between data windows of fixed sizes over the entire data set.
This method is illustrated below:

![Figure 3: Measuring drift magnitude over a data set/stream](Window.png)

We first grow an initial window until it reaches a fixed size (1000 in this case). 
Then we store the model for that window and grow another window right after the end of the previous wwindow.
Once the second window reaches the specified size, we measure the drift magnitude between the models of the 2 windows.
After measuring the drift magnitude, we discard the model of the first window and store the model of the second window.
This process is then repeated with a third window, fourth window, and so on.

This method will provide us with a measure of the drift magnitude at fixed intervals determined by the given window size.
In order to get a measure of the drift magnitude at each instance, we could just continously move the first 2 windows by 1 instance and measure the drift magnitude between the windows each time. However, the cost doing such is a great increase in the time taken to process the entire data set.

## Synthetic Data Generator
I've tested the method above on synthetic data generated with the generator we made earlier last year (2016). 

The generator generates and drift the covariates by creating a Bayesian Network and drifts the network via "*Linear Interpolation*" respectively.
A side effect of this way of drifting the network is it makes the distributions more flat in general.

In order to generate and drift the classes assigned to the different covariates generated by the Bayesian Network, 
the generator uses a tree where each node splits on an attribute of the covariate.
The leaf nodes of the tree then assigns the same class to any covariates that reaches that specific leaf.
In order to drift the assignment of the class, the generator just changes the class assignment of the leaf nodes unitl the desired drift magnitude is reached.

## Test on synthetic data
Here are the results from testing the method on synthtic data with 5 attributes, 5 values per attribute, and 5 classes.
```{r synced_synthetic, echo = FALSE, message = FALSE, warning = FALSE}
PlotWindowSize("COVARIATE", 100000, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
PlotWindowSize("LIKELIHOOD", 100000, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
PlotWindowSize("POSTERIOR", 100000, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
PlotWindowSize("JOINT", 100000, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
```

## Test on synthetic data (out of sync)
Note that the results above sort of represents the "*best case scenario*" as the drift occurs nicely between 2 windows.
The results below represent a less than ideal scenario whhere the drift occurs in the middle of a window.

Also note that using the modification to the windowing method mentioned earlier where the 2 initial windows are just moved over each instance will ensure that the windows at some point will have the abrupt concept drift occur between them.
```{r not_synced_synthetic, echo = FALSE, message = FALSE, warning = FALSE}
PlotWindowSize("COVARIATE", 133332, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
PlotWindowSize("LIKELIHOOD", 133332, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
PlotWindowSize("POSTERIOR", 133332, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
PlotWindowSize("JOINT", 133332, 1, "../../../data_out/synthetic_5Att_5Val/n1000000_m0.7_both/stream/", c())
```

## Comaprison with old static method
The main advatage the old method of analysing drift is the ability to easily analyse the drift 
in more detail by view changes in the observed distribution given different values for the given attributes.
This aids in the analysis of the measured `Likelihood` and `Posterior` drift by separating the drifts of the different distributions that gets averaged together later for analysis.

Other than that, both methods allow for basic analysis of the final mesured drift between 2 windows. In fact, this new method of analysing the entire dataset and obtaining a "*timeline*" of the drift might even be more useful.

# Results
[electicity result](./result_reports/electricity.html)

[airlines result](./result_reports/airlines.html)

# Future Work
A problem with just ploting the measured drift over the entire data set, especially on attribute subset lengths that are smaller than the total number of attributes, is that the plot will become increasingly unreadable as the number of attributes increase. 
A possible solution to this that Bart proposed is to 
combine attrbutes that do not increase in drift greately when combined in the final results.

Furthermore, after viewing the results from `electricity` and `airlines` I believe 
more comprehensive use of "*date*" like attributes to form the windows for the timeline analysis might be useful.
More specifically, give the smallest measure of time for the dataset (for example, day) 
it might be better to increament the larger windows (for example weeks, months, etc) by the smallest measure of time.
This might be much more feasible to run in a reasonable amount of time compared to incrementing by only 1 instance in order to smooth the graph.

Additonally, it might be possible to do the same detailed analysis where the distribution over the different values for the given attribute is analysed separately instead of averaged together but on the new timeline analysis instead.
This can be done by having different drift timelines for each possible value of the given attribute(s).

I should also add the same detailed analysis, similar to the one for posterior, and apply it 
to the covariate drift so that we can observe the change in frequency of the different values of each attribute.
